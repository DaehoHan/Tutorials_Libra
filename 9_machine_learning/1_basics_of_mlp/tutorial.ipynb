{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANN) Basics: Error Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content <a name=\"TOC\"></a>\n",
    "\n",
    "1. [General setups](#setups)\n",
    "\n",
    "2. [Creation and initialization](#creation) \n",
    "\n",
    "3. [Training](#training) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Learning objectives\n",
    "\n",
    "- to create an ANN of arbitrary architecture\n",
    "- to initialize the ANN parameters\n",
    "- to construct an ANN training algorithm\n",
    "- to know the difference between the online and batch training of an ANN\n",
    "- to train ANN and track the progress\n",
    "- to predict the outputs using the ANN and known inputs\n",
    "\n",
    "### B. Use cases\n",
    "\n",
    "- [Deep machine learning: Multilayer Perceptron](#mlp-1)\n",
    "\n",
    "\n",
    "### C. Functions\n",
    "\n",
    "- `liblibra::liblinalg`\n",
    "  - [`pop_submatrix`](#pop_submatrix-1)\n",
    "\n",
    "- `liblibra::libspecialfunctions`\n",
    "  - [`randperm`](#randperm-1) | [also here](#randperm-2)\n",
    "\n",
    "  \n",
    "### D. Classes and class members\n",
    "\n",
    "- `liblibra::libann`\n",
    "  - [`NeuralNetwork`](#NeuralNetwork-1)\n",
    "    - [`Nlayers`](#Nlayers-1)  \n",
    "    - [`Npe`](#Npe-1)\n",
    "    - [`W`](#W-1) | [also here](#W-2)\n",
    "    - [`dW`](#dW-1) | [also here](#dW-2)\n",
    "    - [`B`](#B-1) | [also here](#B-2)\n",
    "    - [`dB`](#dB-1) | [also here](#dB-2)\n",
    "    - [`propagate`](#propagate-1) | [also here](#propagate-2)\n",
    "    - [`back_propagate`](#back_propagate-1) | [also here](#back_propagate-2)\n",
    "    - [`init_weights_biases_uniform`](#init_weights_biases_uniform-1)\n",
    "    - [`init_weights_biases_normal`](#init_weights_biases_normal-1)\n",
    "    - [`train`](#train-1)    \n",
    "\n",
    "- `liblibra::librandom`\n",
    "  - [`Random`](#Random-1)    \n",
    "    - [`normal`](#normal-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. General setups\n",
    "<a name=\"setups\"></a> [Back to TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<float, std::allocator<float> >, std::allocator<std::vector<float, std::allocator<float> > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<float, std::allocator<float> >, std::allocator<std::vector<float, std::allocator<float> > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<float, std::allocator<float> >, std::allocator<std::vector<float, std::allocator<float> > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<double, std::allocator<double> >, std::allocator<std::vector<double, std::allocator<double> > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<double, std::allocator<double> >, std::allocator<std::vector<double, std::allocator<double> > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<double, std::allocator<double> >, std::allocator<std::vector<double, std::allocator<double> > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<std::complex<double>, std::allocator<std::complex<double> > >, std::allocator<std::vector<std::complex<double>, std::allocator<std::complex<double> > > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<std::complex<double>, std::allocator<std::complex<double> > >, std::allocator<std::vector<std::complex<double>, std::allocator<std::complex<double> > > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<std::complex<double>, std::allocator<std::complex<double> > >, std::allocator<std::vector<std::complex<double>, std::allocator<std::complex<double> > > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "import cmath\n",
    "import math\n",
    "import os\n",
    "\n",
    "if sys.platform==\"cygwin\":\n",
    "    from cyglibra_core import *\n",
    "elif sys.platform==\"linux\" or sys.platform==\"linux2\":\n",
    "    from liblibra_core import *\n",
    "import util.libutil as comn\n",
    "\n",
    "from libra_py import units\n",
    "from libra_py import data_outs\n",
    "import matplotlib.pyplot as plt   # plots\n",
    "#matplotlib.use('Agg')\n",
    "#%matplotlib inline \n",
    "\n",
    "import numpy as np\n",
    "#from matplotlib.mlab import griddata\n",
    "\n",
    "plt.rc('axes', titlesize=24)      # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=20)      # fontsize of the x and y labels\n",
    "plt.rc('legend', fontsize=20)     # legend fontsize\n",
    "plt.rc('xtick', labelsize=16)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=16)    # fontsize of the tick labels\n",
    "\n",
    "plt.rc('figure.subplot', left=0.2)\n",
    "plt.rc('figure.subplot', right=0.95)\n",
    "plt.rc('figure.subplot', bottom=0.13)\n",
    "plt.rc('figure.subplot', top=0.88)\n",
    "\n",
    "colors = {}\n",
    "\n",
    "colors.update({\"11\": \"#8b1a0e\"})  # red       \n",
    "colors.update({\"12\": \"#FF4500\"})  # orangered \n",
    "colors.update({\"13\": \"#B22222\"})  # firebrick \n",
    "colors.update({\"14\": \"#DC143C\"})  # crimson   \n",
    "\n",
    "colors.update({\"21\": \"#5e9c36\"})  # green\n",
    "colors.update({\"22\": \"#006400\"})  # darkgreen  \n",
    "colors.update({\"23\": \"#228B22\"})  # forestgreen\n",
    "colors.update({\"24\": \"#808000\"})  # olive      \n",
    "\n",
    "colors.update({\"31\": \"#8A2BE2\"})  # blueviolet\n",
    "colors.update({\"32\": \"#00008B\"})  # darkblue  \n",
    "\n",
    "colors.update({\"41\": \"#2F4F4F\"})  # darkslategray\n",
    "\n",
    "clrs_index = [\"11\", \"21\", \"31\", \"41\", \"12\", \"22\", \"32\", \"13\",\"23\", \"14\", \"24\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creation and initialization\n",
    "<a name=\"creation\"></a> [Back to TOC](#TOC)\n",
    "\n",
    "Create the ANN object with 3 layers: input, 1 hidden, and 1 output layers. \n",
    "<a name=\"NeuralNetwork-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN = NeuralNetwork( Py2Cpp_int( [2, 5, 1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then check that:\n",
    "    \n",
    "* there are 3 layers\n",
    "* the input layers has 2 neurons\n",
    "* the hidden layer has 5 neurons\n",
    "* the output has 1 neuron\n",
    "<a name=\"Nlayers-1\"></a><a name=\"Npe-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers = 3\n",
      "Input layer dimension = 2\n",
      "Hidden layer dimension = 5\n",
      "Output layer dimension = 1\n"
     ]
    }
   ],
   "source": [
    "print(F\"Number of layers = {ANN.Nlayers}\")\n",
    "print(F\"Input layer dimension = {ANN.Npe[0]}\")\n",
    "print(F\"Hidden layer dimension = {ANN.Npe[1]}\")\n",
    "print(F\"Output layer dimension = {ANN.Npe[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation simply creates a collection of the weights and biases, that are stored in the matrices `W` and `B` correspondingly.\n",
    "\n",
    "Also, this operation initializes the storage for the deltas of these parameters - `dW` and `dB`\n",
    "\n",
    "Note that matrices `W[0]` and `B[0]` are irrelevant (junk), are not used and are only needed for the consistency of the implementation with the common ways the ANN theory is described in the literature\n",
    "<a name=\"W-1\"></a><a name=\"B-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[0]\n",
      "1.0  0.0  \n",
      "0.0  1.0  \n",
      "W[1]\n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "W[2]\n",
      "0.0  0.0  0.0  0.0  0.0  \n",
      "B[0]\n",
      "0.0  \n",
      "0.0  \n",
      "B[1]\n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "0.0  \n",
      "B[2]\n",
      "0.0  \n"
     ]
    }
   ],
   "source": [
    "print(\"W[0]\"); data_outs.print_matrix(ANN.W[0])\n",
    "print(\"W[1]\"); data_outs.print_matrix(ANN.W[1])\n",
    "print(\"W[2]\"); data_outs.print_matrix(ANN.W[2])\n",
    "\n",
    "print(\"B[0]\"); data_outs.print_matrix(ANN.B[0])\n",
    "print(\"B[1]\"); data_outs.print_matrix(ANN.B[1])\n",
    "print(\"B[2]\"); data_outs.print_matrix(ANN.B[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider the AND gate: \n",
    "\n",
    "\n",
    "| Input A | Input B |  Output A and B |\n",
    "| --- | --- | --- |\n",
    "|  0  |  0  |  0  |\n",
    "|  0  |  1  |  0  |\n",
    "|  0  |  1  |  0  |\n",
    "|  1  |  1  |  1  |\n",
    "\n",
    "For the numerical convenience, the inputs and outputs are rescaled down to the [0.0, 0.5] range\n",
    "\n",
    "The AND truth table can be summarized as 4 inputs and 4 outputs. Each input and output constitute a column of the corresponding matrix. The length of each column (the number of rows in the matrices) corresponds to the dimensionalty of the input (2 - for the A and B) and output (A and B representation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = MATRIX(2, 4)\n",
    "outputs = MATRIX(1, 4)\n",
    "\n",
    "# Pattern 0\n",
    "inputs.set(0, 0, 0.0)\n",
    "inputs.set(1, 0, 0.0)\n",
    "outputs.set(0, 0, 0.0)\n",
    "\n",
    "# Pattern 1\n",
    "inputs.set(0, 1, 0.0)\n",
    "inputs.set(1, 1, 0.5)\n",
    "outputs.set(0, 1, 0.0)\n",
    "\n",
    "# Pattern 2\n",
    "inputs.set(0, 2, 0.5)\n",
    "inputs.set(1, 2, 0.0)\n",
    "outputs.set(0, 2, 0.0)\n",
    "\n",
    "# Pattern 3\n",
    "inputs.set(0, 3, 0.5)\n",
    "inputs.set(1, 3, 0.5)\n",
    "outputs.set(0, 3, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we need to initialize the values of the weights and biases of the ANN\n",
    "<a name=\"Random-1\"></a><a name=\"normal-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[0]\n",
      "1.0  0.0  \n",
      "0.0  1.0  \n",
      "W[1]\n",
      "-0.07467980759493115  0.11535009504088899  \n",
      "-0.09194019431931354  -0.05608876287839962  \n",
      "0.021204018696542492  0.11335481251639841  \n",
      "-0.01365116321426383  -0.04694757874421607  \n",
      "0.2074992501529765  0.09111487578906989  \n",
      "W[2]\n",
      "0.0688396129970422  -0.07374125095097554  0.12718422746755392  0.04758712038343996  -0.016865861519680028  \n",
      "B[0]\n",
      "0.0  \n",
      "0.0  \n",
      "B[1]\n",
      "0.21761276937257526  \n",
      "0.04667804195371106  \n",
      "0.02300427344265035  \n",
      "-0.029876880048314982  \n",
      "-0.013555923647332428  \n",
      "B[2]\n",
      "0.09641082564507389  \n"
     ]
    }
   ],
   "source": [
    "rnd = Random()\n",
    "\n",
    "for L in range(1, ANN.Nlayers):\n",
    "    for i in range(ANN.Npe[L]):\n",
    "        for j in range(ANN.Npe[L-1]):\n",
    "            ANN.W[L].set(i, j, 0.1*rnd.normal())\n",
    "        ANN.B[L].set(i, 0, 0.1*rnd.normal() )\n",
    "        \n",
    "print(\"W[0]\"); data_outs.print_matrix(ANN.W[0])\n",
    "print(\"W[1]\"); data_outs.print_matrix(ANN.W[1])\n",
    "print(\"W[2]\"); data_outs.print_matrix(ANN.W[2])\n",
    "\n",
    "print(\"B[0]\"); data_outs.print_matrix(ANN.B[0])\n",
    "print(\"B[1]\"); data_outs.print_matrix(ANN.B[1])\n",
    "print(\"B[2]\"); data_outs.print_matrix(ANN.B[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation can also be done with the help of the auxiliary function `init_weights_biases_normal` or `init_weights_biases_uniform`:\n",
    "\n",
    "    void init_weights_biases_uniform(Random& rnd, double left_w, double right_w, double left_b, double right_b);\n",
    "    void init_weights_biases_normal(Random& rnd, double scaling_w, double shift_w, double scaling_b, double shift_b);\n",
    " \n",
    "<a name=\"init_weights_biases_normal-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[0]\n",
      "1.0  0.0  \n",
      "0.0  1.0  \n",
      "W[1]\n",
      "0.12444313572962284  -0.045443184771789685  \n",
      "-0.016257630548090516  -0.011498667603197187  \n",
      "0.019002906005639406  0.17753078630284458  \n",
      "-0.05183249870320597  0.0159620186499942  \n",
      "0.11555013892191231  -0.19511227958004979  \n",
      "W[2]\n",
      "0.1962570269469692  0.06204127313536661  0.11199818560287973  -0.039360286341801026  -0.0017918247500841996  \n",
      "B[0]\n",
      "0.0  \n",
      "0.0  \n",
      "B[1]\n",
      "0.11389210692325298  \n",
      "0.09354658686501437  \n",
      "-0.11337431543870458  \n",
      "0.21521497810771573  \n",
      "-0.05955214956328814  \n",
      "B[2]\n",
      "-0.1319612639552035  \n"
     ]
    }
   ],
   "source": [
    "ANN.init_weights_biases_normal(rnd, 0.1, 0.0, 0.1, 0.0)\n",
    "\n",
    "print(\"W[0]\"); data_outs.print_matrix(ANN.W[0])\n",
    "print(\"W[1]\"); data_outs.print_matrix(ANN.W[1])\n",
    "print(\"W[2]\"); data_outs.print_matrix(ANN.W[2])\n",
    "\n",
    "print(\"B[0]\"); data_outs.print_matrix(ANN.B[0])\n",
    "print(\"B[1]\"); data_outs.print_matrix(ANN.B[1])\n",
    "print(\"B[2]\"); data_outs.print_matrix(ANN.B[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the weights and biases using random numbers sampled from a uniform distribution.\n",
    "<a name=\"init_weights_biases_uniform-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W[0]\n",
      "1.0  0.0  \n",
      "0.0  1.0  \n",
      "W[1]\n",
      "0.005786912285623558  -0.03757073643504211  \n",
      "-0.04014041458263082  0.005311267685755738  \n",
      "0.09958013631383897  0.08877924642934429  \n",
      "0.08163965948933721  -0.0494387475538248  \n",
      "-0.03441802250892763  0.03828138650314947  \n",
      "W[2]\n",
      "0.08378060952936328  0.03417718062837477  -0.07769754816670787  -0.05893732679958331  -0.04999567556660422  \n",
      "B[0]\n",
      "0.0  \n",
      "0.0  \n",
      "B[1]\n",
      "0.0012398463214001731  \n",
      "-0.0158380354362717  \n",
      "-0.04609113798760396  \n",
      "-0.0684166976103637  \n",
      "0.09977275384579448  \n",
      "B[2]\n",
      "-0.07770523725901975  \n"
     ]
    }
   ],
   "source": [
    "ANN.init_weights_biases_uniform(rnd, -0.1, 0.1, -0.1, 0.1)\n",
    "\n",
    "print(\"W[0]\"); data_outs.print_matrix(ANN.W[0])\n",
    "print(\"W[1]\"); data_outs.print_matrix(ANN.W[1])\n",
    "print(\"W[2]\"); data_outs.print_matrix(ANN.W[2])\n",
    "\n",
    "print(\"B[0]\"); data_outs.print_matrix(ANN.B[0])\n",
    "print(\"B[1]\"); data_outs.print_matrix(ANN.B[1])\n",
    "print(\"B[2]\"); data_outs.print_matrix(ANN.B[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "<a name=\"training\"></a> [Back to TOC](#TOC)\n",
    "\n",
    "To train the ANN on a given set of patterns, we use two key functions: `propagate` and `back_propagate`, which take the signatures:\n",
    "\n",
    "    vector<MATRIX> propagate(MATRIX& input);\n",
    "    double back_propagate(vector<MATRIX>& Y, MATRIX& target);\n",
    "\n",
    "The `propagate` function takes a given input (which could be as many patterns as needed) and computes the outputs on each layer for each pattern. The results are returned as the lists of matrices.\n",
    "\n",
    "The `back_propagate` function takes the output in each layer (as returned by the `propagate` function) as well as the expected target output and computes the error on each layer (and the corresponding derivatives of the weights and biases), starting from the last (output) layer and working its way down to the first one. The error is thus propagated backwards, hence the name. \n",
    "\n",
    "As a result, the procedure updates the `dW` and `dB` values stored internally in the ANN object. The function also returns the error in the last layer to facilitae the tracking of the progress.\n",
    "\n",
    "Note that if there are many patterns are given, the `dW` and `dB` variables are computed as the average over those values over all the patterns.\n",
    "<a name=\"dW-1\"></a><a name=\"dB-1\"></a><a name=\"propagate-1\"></a><a name=\"back_propagate-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error = 0.04519214277195703\n",
      "dW[0]\n",
      "0.0  0.0  \n",
      "0.0  0.0  \n",
      "dW[1]\n",
      "0.006981422098446493  0.006966319701928951  \n",
      "0.0028453098207850695  0.0028396355800587676  \n",
      "-0.006462613642992636  -0.006448841305310657  \n",
      "-0.004899931056470072  -0.004884864656572826  \n",
      "-0.004125887118961536  -0.0041134794537402855  \n",
      "dW[2]\n",
      "-0.0023874349345629504  -0.0061559730794529255  0.006207818630260986  -0.011350855168443252  0.020743749447816894  \n",
      "dB[0]\n",
      "0.0  \n",
      "0.0  \n",
      "dB[1]\n",
      "0.017211631886592204  \n",
      "0.007015850018761089  \n",
      "-0.015935499196266023  \n",
      "-0.012070339046558558  \n",
      "-0.010167499375128945  \n",
      "dB[2]\n",
      "0.20547487545388454  \n"
     ]
    }
   ],
   "source": [
    "Y = ANN.propagate(inputs)\n",
    "res = ANN.back_propagate(Y, outputs)\n",
    "print(F\"Error = {res}\")\n",
    "\n",
    "print(\"dW[0]\"); data_outs.print_matrix(ANN.dW[0])\n",
    "print(\"dW[1]\"); data_outs.print_matrix(ANN.dW[1])\n",
    "print(\"dW[2]\"); data_outs.print_matrix(ANN.dW[2])\n",
    "\n",
    "print(\"dB[0]\"); data_outs.print_matrix(ANN.dB[0])\n",
    "print(\"dB[1]\"); data_outs.print_matrix(ANN.dB[1])\n",
    "print(\"dB[2]\"); data_outs.print_matrix(ANN.dB[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can formulate a simple procedure perform the simple gradient descent optimization of the weights and biases.\n",
    "\n",
    "Note that `dW` and `dB` are the negative gradients of the error w.r.t. to those parameters. So, in the gradiens descent algorithm, these come with the \"+\" sign\n",
    "\n",
    "Naturally, we don't want to plot all the stuff, only once in a while. \n",
    "<a name=\"W-2\"></a><a name=\"B-2\"></a><a name=\"dW-2\"></a><a name=\"dB-2\"></a><a name=\"propagate-2\"></a><a name=\"back_propagate-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0  error = 0.023440060714565544\n",
      "0.1251404016968137  0.1261748411974405  0.12405553896998396  0.12509934878777298  \n",
      "epoch = 1  error = 0.022877031224618205\n",
      "0.12079707098479159  0.12623134245508394  0.12440946486007147  0.12984834098216064  \n",
      "epoch = 2  error = 0.021931851676821348\n",
      "0.11343032688733137  0.12672544345056644  0.12492340040874216  0.13816691642502021  \n",
      "epoch = 3  error = 0.020131881373898083\n",
      "0.09843806164998027  0.12770045604872832  0.12576847769806593  0.15468871644234208  \n",
      "epoch = 4  error = 0.017154659528963057\n",
      "0.07001100954140131  0.1289552383265973  0.12691086082897943  0.18440545840196135  \n",
      "epoch = 5  error = 0.013521031474355027\n",
      "0.025989260420289657  0.12991805799449047  0.12794685345462875  0.2275230082453813  \n",
      "epoch = 6  error = 0.010612814718493173\n",
      "-0.024970551805868203  0.13028236876132387  0.12860830622325864  0.27468822212148164  \n",
      "epoch = 7  error = 0.009141472331890901\n",
      "-0.0682336216531886  0.13029595092996296  0.12901914858518704  0.31331048234882913  \n",
      "epoch = 8  error = 0.008644745505395353\n",
      "-0.09675680442569876  0.13020176380292442  0.1292775791379453  0.33834947239390256  \n",
      "epoch = 9  error = 0.008522056211249579\n",
      "-0.11270023454961922  0.13007836191827588  0.12940349380064142  0.35231972922796073  \n",
      "epoch = 10  error = 0.008496204074936314\n",
      "-0.12082531133591491  0.1299497457725412  0.12942945739963427  0.3595295195745458  \n",
      "epoch = 11  error = 0.008486764517291367\n",
      "-0.12477676557243743  0.1298263270084952  0.12939380932692476  0.36315248692445073  \n",
      "epoch = 12  error = 0.008477255474950231\n",
      "-0.12665933815037145  0.1297115788160765  0.12932453258745563  0.364998046491954  \n",
      "epoch = 13  error = 0.008466045648069118\n",
      "-0.1275531597828281  0.12960542570081227  0.1292382364970696  0.3659907896075497  \n",
      "epoch = 14  error = 0.008453841875606091\n",
      "-0.12798321441620575  0.12950654068173148  0.1291439461595652  0.3665798781160406  \n",
      "epoch = 15  error = 0.008441288171703228\n",
      "-0.12819799157985737  0.12941344029711588  0.12904637812196942  0.3669782946561609  \n",
      "epoch = 16  error = 0.008428735364583377\n",
      "-0.12831348225314895  0.12932485956472864  0.12894793952856273  0.3672858606679786  \n",
      "epoch = 17  error = 0.008416341215847747\n",
      "-0.1283833825871902  0.12923981517449226  0.1288498205027998  0.3675491384236687  \n",
      "epoch = 18  error = 0.008404162910230733\n",
      "-0.12843240263393743  0.12915756462963746  0.12875256951649855  0.3677898620477224  \n",
      "epoch = 19  error = 0.008392209774888795\n",
      "-0.12847185170227976  0.1290775453577819  0.1286563945348703  0.36801825524340365  \n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "steps_per_epoch = 1000\n",
    "dt = 0.01\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    res, Y = 0.0, None\n",
    "    for i in range(steps_per_epoch):\n",
    "    \n",
    "        for L in range(ANN.Nlayers):\n",
    "            ANN.W[L] = ANN.W[L] + dt * ANN.dW[L]\n",
    "            ANN.B[L] = ANN.B[L] + dt * ANN.dB[L]\n",
    "\n",
    "        Y = ANN.propagate(inputs)\n",
    "        res = ANN.back_propagate(Y, outputs)\n",
    "        \n",
    "    print(F\"epoch = {epoch}  error = {res}\")\n",
    "    \n",
    "    data_outs.print_matrix(Y[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After enough steps and cycles, we can compute the ANN recall (prediction) using the current state of the ANN parameters.\n",
    "\n",
    "As an example, we use the input that was also used in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ANN.propagate(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.12847185170227976  0.1290775453577819  0.1286563945348703  0.36801825524340365  \n"
     ]
    }
   ],
   "source": [
    "data_outs.print_matrix(Y[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the results are pretty close to our expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we have utilized all of our training examples in each step. This is called **batch** training\n",
    "\n",
    "However, sometimes it is adwantageous to use randomly selected subsets of the training examples in each step. This is called **online** training, and the number of examples presented at each time is called **epoch size**. In the above example of the batch training, we used an epoch size of 4.\n",
    "\n",
    "Let's consider smaller epoch sizes.\n",
    "\n",
    "In order to implement such a functionality, we need a procedure to select random sequences of numbers. This can be done with the help of `randperm` function, which takes the signature:\n",
    "\n",
    "    int randperm(int size,int of_size,vector<int>& result)\n",
    "    \n",
    "For instance, if we want to create a random sequence of 3 numbers from 5 numbers [0, 1, 2, 3, 4], we do:\n",
    "<a name=\"randperm-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "res = intList()\n",
    "randperm(3, 5, res)\n",
    "print( Cpp2Py(res) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to formulate the algorithm.\n",
    "\n",
    "<a name=\"pop_submatrix-1\"></a>\n",
    "Note how we use the `pop_submatrix` function to take certain columns (as defined by the `subset` variable) out of the full matrix of all inputs. We do such \"extraction\" for both inputs and outputs, congruently.\n",
    "<a name=\"randperm-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0  error = 0.03823947430633084\n",
      "0.1252214825107677  0.12949024635898065  \n",
      "epoch = 1  error = 0.037252160169705985\n",
      "0.13301653599205693  0.1197154034514969  \n",
      "epoch = 2  error = 0.008482091112683457\n",
      "0.13380230949427305  0.1265910993108623  \n",
      "epoch = 3  error = 0.031340193420606954\n",
      "0.13213619266623478  0.17151742775285705  \n",
      "epoch = 4  error = 0.022018538568777207\n",
      "0.07394119111043716  0.21258591799934606  \n",
      "epoch = 5  error = 0.017121485645586906\n",
      "0.23845297783791403  0.008893693298789666  \n",
      "epoch = 6  error = 0.015762960491216046\n",
      "0.14083840252468122  0.292114487423582  \n",
      "epoch = 7  error = 0.005340491264554217\n",
      "-0.08496770977843712  0.11892204738072815  \n",
      "epoch = 8  error = 0.010621065936320971\n",
      "0.12621924425300274  0.33704919108674325  \n",
      "epoch = 9  error = 0.009597623762396534\n",
      "0.13726153725449614  0.13982047575415663  \n",
      "epoch = 10  error = 0.008965033872500968\n",
      "0.36175077571492065  0.12941131123119684  \n",
      "epoch = 11  error = 0.008742237662892307\n",
      "0.1315795352113171  0.3671249588329584  \n",
      "epoch = 12  error = 0.00847041259878193\n",
      "0.10836835492745632  -0.14878827254002439  \n",
      "epoch = 13  error = 0.008174649969730356\n",
      "-0.11969345855967556  0.13554363082396995  \n",
      "epoch = 14  error = 0.008753716516054822\n",
      "0.37807127969179455  0.14194454279127006  \n",
      "epoch = 15  error = 0.00834220349759639\n",
      "-0.13970984274204803  0.11768591177952342  \n",
      "epoch = 16  error = 0.00873526443878667\n",
      "0.3817236108064251  0.14474720554980214  \n",
      "epoch = 17  error = 0.008509685544992474\n",
      "-0.12335495488411415  0.13719437774744292  \n",
      "epoch = 18  error = 0.008512316492085326\n",
      "-0.12104544182374051  0.1392740714635598  \n",
      "epoch = 19  error = 0.008373767286416766\n",
      "0.36359860889008777  0.12202347990836773  \n",
      "-0.1370252543349552  0.12202347990836773  0.12247408544858754  0.36359860889008777  \n"
     ]
    }
   ],
   "source": [
    "ANN2 = NeuralNetwork( Py2Cpp_int( [2, 5, 1] ) )\n",
    "ANN2.init_weights_biases_uniform(rnd, -0.1, 0.1, -0.1, 0.1)\n",
    "\n",
    "\n",
    "n_epochs = 20\n",
    "steps_per_epoch = 1000\n",
    "epoch_size = 2\n",
    "n_patterns = 4\n",
    "dt = 0.01\n",
    "\n",
    "input_subset = MATRIX(2, epoch_size)\n",
    "output_subset = MATRIX(1, epoch_size)\n",
    "subset = intList()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    res, Y = 0.0, None\n",
    "    for i in range(steps_per_epoch):    \n",
    "        for L in range(ANN.Nlayers):\n",
    "            ANN2.W[L] = ANN2.W[L] + dt * ANN2.dW[L]\n",
    "            ANN2.B[L] = ANN2.B[L] + dt * ANN2.dB[L]\n",
    "            \n",
    "        # Make a random selection of the training patterns\n",
    "        randperm(epoch_size, n_patterns, subset)\n",
    "        \n",
    "        # Extract the corresponding matrices from the inputs and outputs\n",
    "        pop_submatrix(inputs, input_subset, Py2Cpp_int( [0, 1] ), subset )\n",
    "        pop_submatrix(outputs, output_subset, Py2Cpp_int( [0] ), subset )\n",
    "\n",
    "        Y = ANN2.propagate(input_subset)\n",
    "        res = ANN2.back_propagate(Y, output_subset)\n",
    "        \n",
    "    print(F\"epoch = {epoch}  error = {res}\")\n",
    "    \n",
    "    data_outs.print_matrix(Y[2])\n",
    "    \n",
    "Y = ANN2.propagate(inputs)\n",
    "data_outs.print_matrix(Y[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, the above simple procedure can be run as a sinlge function `train`\n",
    "<a name=\"train-1\"></a><a name=\"mlp-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.11978020876310912  0.13074530990140731  0.13483606915940663  0.3696247393767548  \n"
     ]
    }
   ],
   "source": [
    "ANN3 = NeuralNetwork( Py2Cpp_int( [2, 5, 1] ) )\n",
    "ANN3.init_weights_biases_uniform(rnd, -0.1, 0.1, -0.1, 0.1)\n",
    "\n",
    "params = { \"num_epochs\":20, \n",
    "           \"steps_per_epoch\":1000, \n",
    "           \"epoch_size\":2, \"learning_rate\":0.01, \n",
    "           \"verbosity\":1 }\n",
    "\n",
    "ANN3.train(rnd, params, inputs, outputs )\n",
    "\n",
    "Y = ANN3.propagate(inputs)\n",
    "data_outs.print_matrix(Y[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Train an ANN to learn the exclusinve OR (XOR) gate:\n",
    "\n",
    "| Input A | Input B |  Output A and B |\n",
    "| --- | --- | --- |\n",
    "|  0  |  0  |  0  |\n",
    "|  0  |  1  |  1  |\n",
    "|  0  |  1  |  1  |\n",
    "|  1  |  1  |  0  |\n",
    "\n",
    "Experiment with the ANN architecture and training parameters. Can you make the ANN with no hidden layers to learn this pattern?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Train an ANN to learn the quadratic function $y(x) = x^2$ on the [0, 5] interval. \n",
    "\n",
    "Hint: keep in mind that the output of the $tanh(x)$ function can be in the [-1, 1] interval, so you need to transform target y values into that interval. Even better, to something like [-0.5, 0.5]\n",
    "\n",
    "Also, the best learning happens where the slope of the transfer function isn't too close to zero, so it is a good idea to convert the input variables into another range, e.g. [-1, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
