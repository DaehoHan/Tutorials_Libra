{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1. Gradient descent optimization\n",
    "\n",
    "In this tutorial, we will learn:\n",
    "\n",
    "* how to optimize a function using the gradient descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<int, std::allocator<int> >, std::allocator<std::vector<int, std::allocator<int> > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<float, std::allocator<float> >, std::allocator<std::vector<float, std::allocator<float> > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<float, std::allocator<float> >, std::allocator<std::vector<float, std::allocator<float> > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<float, std::allocator<float> >, std::allocator<std::vector<float, std::allocator<float> > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<double, std::allocator<double> >, std::allocator<std::vector<double, std::allocator<double> > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<double, std::allocator<double> >, std::allocator<std::vector<double, std::allocator<double> > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<double, std::allocator<double> >, std::allocator<std::vector<double, std::allocator<double> > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for std::vector<std::vector<std::complex<double>, std::allocator<std::complex<double> > >, std::allocator<std::vector<std::complex<double>, std::allocator<std::complex<double> > > > > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n",
      "/home/alexey/miniconda2/envs/py37/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: to-Python converter for boost::python::detail::container_element<std::vector<std::vector<std::complex<double>, std::allocator<std::complex<double> > >, std::allocator<std::vector<std::complex<double>, std::allocator<std::complex<double> > > > >, unsigned long, boost::python::detail::final_vector_derived_policies<std::vector<std::vector<std::complex<double>, std::allocator<std::complex<double> > >, std::allocator<std::vector<std::complex<double>, std::allocator<std::complex<double> > > > >, false> > already registered; second conversion method ignored.\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cmath\n",
    "import math\n",
    "import os\n",
    "\n",
    "if sys.platform==\"cygwin\":\n",
    "    from cyglibra_core import *\n",
    "elif sys.platform==\"linux\" or sys.platform==\"linux2\":\n",
    "    from liblibra_core import *\n",
    "import util.libutil as comn\n",
    "from libra_py import data_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theory\n",
    "\n",
    "The approach is pretty straightforward - to reach the minimum of the function, we just need to follow oppositely to its gradient, so the update steps are:\n",
    "\n",
    "$$\n",
    "q_\\alpha (n+1) = q_\\alpha (n) - \\frac{\\partial f}{\\partial q_\\alpha} (q_n) * \\Delta\n",
    "$$\n",
    "\n",
    "Here, $\\Delta$ is the step size, $\\alpha$ is index of the degree of freedom (DOF), and $q$ refers to a multidimensional coordinates on which the function depends.\n",
    "\n",
    "Obviously, the approach requires a choice of some starting point, $q(0)$, which may be non-trivial, especially for fast varying functions (with multiple minima) and in high dimensions.\n",
    "\n",
    "The choice of the stepping parameter $\\Delta$ is another question - it shall be neither too large (so the procedure does converge), nor too small (so it converges efficiently)\n",
    "\n",
    "\n",
    "The algorithm is available in Libra via the function \n",
    "\n",
    "`grad_descent(grad_function, dof, funct_params, grad_tol, step_size, max_steps)` \n",
    "\n",
    "defined in the **opt** library. The function takes the following parameters:\n",
    "\n",
    "\n",
    "  * `grad_function` - shoud be Python function (representing $f$ in the above theory) of the signature\n",
    "    `py_funct(MATRIX& dof, bp::object params)` \n",
    "    This function computes the gradient of thefunction which we minimize and should return it as a *MATRIX* type\n",
    "      \n",
    "    Here:\n",
    "      - `dof` is of type *MATRIX* and represents the arguments of the function ($q$ in the above theory)\n",
    "      - `params` is of type Python object (a function, a dictionary, a list, a class, etc), which stores\n",
    "          the parameters of the function (so it is advisable to use a Python dictionary for it)\n",
    "          \n",
    "  * `dof` - represents the arguments of the function and should be of type *MATRIX*. Note that this variable\n",
    "    would contain the final results of the calculations, so the end and starting values may be different.\n",
    "    \n",
    "  * `funct_params` - should be of the *Python object* type and contains the parameters passed to the `grad_function`\n",
    "\n",
    "  * `grad_tol` - the tolerance of the gradient value. It is one of the stopping critetion: the iterations stop as soon ass the max value of a projection of the gradient $ \\frac{\\partial f}{\\partial q_\\alpha} (q_n) $ vector drops below this level\n",
    "  \n",
    "  * `step_size` - is the algorithm parameter, which corresponds to $\\Delta$ in the equation above\n",
    "  \n",
    "  * `max_steps` - is yet another stopping criterion. The iterations stop after that many steps, no matter whether the gradient criterion is met or not.\n",
    "  \n",
    "  \n",
    "## 2. Examples\n",
    "\n",
    "In all cases below, we shall define our function to optimize. We do this in accordiance with the expectation from the above function signatures. Then we essentially can the `grad_descent` function with the suitable parameters and get the results.\n",
    "\n",
    "### 2.1. Example 1.\n",
    "\n",
    "Here is our model function of 3 variables (3 DOFs):\n",
    "\n",
    "$$ E(x,y,z) = \\frac{1}{2} (x-1)^2 + (y+2)^2 + z^2 $$\n",
    "\n",
    "To match the format, we represent the variables $x, y, z$ as a 3D vector:  \n",
    "\n",
    "\n",
    "$$\n",
    "q = \\begin{pmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "It is clear that the minimum of this function will be at $q = (1, -2, 0)^T$\n",
    "\n",
    "The gradients of the function are given by:\n",
    "\n",
    "$$\n",
    "\\nabla E = \\begin{pmatrix}\n",
    "\\frac{\\partial E}{\\partial x} \\\\\n",
    "\\frac{\\partial E}{\\partial y} \\\\\n",
    "\\frac{\\partial E}{\\partial z} \\\\\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "x-1 \\\\\n",
    "2(y+2) \\\\\n",
    "2z \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This encoded in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1(q, params):\n",
    "    \"\"\"\n",
    "    q = (x,y,z)\n",
    "    E = 1/2* ((x-1)^2 + (y+2)^2 + z^2)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    grd = MATRIX(3,1) \n",
    "    x,y,z = q.get(0), q.get(1), q.get(2)\n",
    "\n",
    "    grd.set(0, x-1.0)\n",
    "    grd.set(1, y+2.0)\n",
    "    grd.set(2, z)\n",
    "\n",
    "    return grd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define a test function that would initialize the guessed minimum, at $q = (1, 1, 1)^T$ and would make 10 steps of relative size 1.0 and print out the resulting values of the guess minima.\n",
    "\n",
    "Note that in this example, the function doesn't take any parameters, but we still need to satisfy the functions signatures, so we pass an empty dictionary of parameters.\n",
    "\n",
    "This function prints out the resulting matrix to the Jupyter output via the use of `data_outs.print_matrix` functions. The matrices can also be printed out to the terminal (see behind the Jupyter, perhaps) with the member function of the `MATRIX` class, the `show_matrix` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    q = MATRIX(3,1)\n",
    "    q.set(0, 1)\n",
    "    q.set(1, 1)\n",
    "    q.set(2, 1)\n",
    "\n",
    "    params = {}    \n",
    "           \n",
    "    qopt = grad_descent(model1, q, params, 1e-6, 1.0, 10)\n",
    "\n",
    "    print(\"The optimized positions are:\"); data_outs.print_matrix(qopt)\n",
    "    qopt.show_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the above function didn't produce anything yet - since it is just a definition.\n",
    "\n",
    "And here we call the computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimized positions are:\n",
      "1.0  \n",
      "-2.0  \n",
      "0.0  \n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what we expected to see\n",
    "\n",
    "### 2.2. Example 2\n",
    "\n",
    "Now, lets modify our function above to do just 1 step per function call, print out the information more frequently so that we could see how the optimization takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2():\n",
    "    q = MATRIX(3,1)\n",
    "    q.set(0, 1)\n",
    "    q.set(1, 1)\n",
    "    q.set(2, 1)\n",
    "\n",
    "    params = {}    \n",
    "        \n",
    "    tol = 1e-5\n",
    "    err = 2*tol\n",
    "    i = 0\n",
    "    \n",
    "    while err > tol and i<50:\n",
    "            \n",
    "        q = grad_descent(model1, q, params, 1e-6, 0.5, 1)\n",
    "        \n",
    "        grd = model1(q, params)        \n",
    "        err = max( abs(grd.get(0)), abs(grd.get(1)), abs(grd.get(2)))\n",
    "        print(F\"step {i} gradient = {grd.get(0), grd.get(1), grd.get(2) } \\\n",
    "        The optimized positions are: {q.get(0), q.get(1), q.get(2)}\")\n",
    "        \n",
    "        i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 gradient = (0.0, 1.5, 0.5)         The optimized positions are: (1.0, -0.5, 0.5)\n",
      "step 1 gradient = (0.0, 0.75, 0.25)         The optimized positions are: (1.0, -1.25, 0.25)\n",
      "step 2 gradient = (0.0, 0.375, 0.125)         The optimized positions are: (1.0, -1.625, 0.125)\n",
      "step 3 gradient = (0.0, 0.1875, 0.0625)         The optimized positions are: (1.0, -1.8125, 0.0625)\n",
      "step 4 gradient = (0.0, 0.09375, 0.03125)         The optimized positions are: (1.0, -1.90625, 0.03125)\n",
      "step 5 gradient = (0.0, 0.046875, 0.015625)         The optimized positions are: (1.0, -1.953125, 0.015625)\n",
      "step 6 gradient = (0.0, 0.0234375, 0.0078125)         The optimized positions are: (1.0, -1.9765625, 0.0078125)\n",
      "step 7 gradient = (0.0, 0.01171875, 0.00390625)         The optimized positions are: (1.0, -1.98828125, 0.00390625)\n",
      "step 8 gradient = (0.0, 0.005859375, 0.001953125)         The optimized positions are: (1.0, -1.994140625, 0.001953125)\n",
      "step 9 gradient = (0.0, 0.0029296875, 0.0009765625)         The optimized positions are: (1.0, -1.9970703125, 0.0009765625)\n",
      "step 10 gradient = (0.0, 0.00146484375, 0.00048828125)         The optimized positions are: (1.0, -1.99853515625, 0.00048828125)\n",
      "step 11 gradient = (0.0, 0.000732421875, 0.000244140625)         The optimized positions are: (1.0, -1.999267578125, 0.000244140625)\n",
      "step 12 gradient = (0.0, 0.0003662109375, 0.0001220703125)         The optimized positions are: (1.0, -1.9996337890625, 0.0001220703125)\n",
      "step 13 gradient = (0.0, 0.00018310546875, 6.103515625e-05)         The optimized positions are: (1.0, -1.99981689453125, 6.103515625e-05)\n",
      "step 14 gradient = (0.0, 9.1552734375e-05, 3.0517578125e-05)         The optimized positions are: (1.0, -1.999908447265625, 3.0517578125e-05)\n",
      "step 15 gradient = (0.0, 4.57763671875e-05, 1.52587890625e-05)         The optimized positions are: (1.0, -1.9999542236328125, 1.52587890625e-05)\n",
      "step 16 gradient = (0.0, 2.288818359375e-05, 7.62939453125e-06)         The optimized positions are: (1.0, -1.9999771118164062, 7.62939453125e-06)\n",
      "step 17 gradient = (0.0, 1.1444091796875e-05, 3.814697265625e-06)         The optimized positions are: (1.0, -1.9999885559082031, 3.814697265625e-06)\n",
      "step 18 gradient = (0.0, 5.7220458984375e-06, 1.9073486328125e-06)         The optimized positions are: (1.0, -1.9999942779541016, 1.9073486328125e-06)\n"
     ]
    }
   ],
   "source": [
    "test2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, this example allows you to introduce more control of the stopping of the optimization process. Note that the tolerance defined by the `tol` variable matters only in our Python loop, but the tolerance passed to the function `grad_descent` doesn't matter for 1 step.\n",
    "\n",
    "See how even though we considered doing up to 50 steps, we have met the convergence criterion a bit earlier and exited our loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
